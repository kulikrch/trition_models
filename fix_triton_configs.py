"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π Triton –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–µ CIFAR-10 –º–æ–¥–µ–ª–∏
"""
import os
import torch
import onnx
import sys
from working_demo import SimpleCNN

def analyze_pytorch_model():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç PyTorch –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    print("üîç –ê–Ω–∞–ª–∏–∑ PyTorch –º–æ–¥–µ–ª–∏...")
    
    model_path = "models/working_model.pth"
    if not os.path.exists(model_path):
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return None
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = SimpleCNN(num_classes=10)
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # –°–æ–∑–¥–∞–µ–º traced –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    dummy_input = torch.randn(1, 3, 32, 32)
    
    with torch.no_grad():
        output = model(dummy_input)
    
    print(f"‚úÖ PyTorch –º–æ–¥–µ–ª—å:")
    print(f"   Input shape: {list(dummy_input.shape)}")
    print(f"   Output shape: {list(output.shape)}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º traced –º–æ–¥–µ–ª—å
    try:
        traced_model = torch.jit.trace(model, dummy_input)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤
        graph = traced_model.graph
        inputs = list(graph.inputs())
        outputs = list(graph.outputs())
        
        print(f"   Traced inputs: {len(inputs)}")
        print(f"   Traced outputs: {len(outputs)}")
        
        return {
            'input_shape': [3, 32, 32],
            'output_shape': [10],
            'input_name': 'input__0',  # Standard –¥–ª—è PyTorch –≤ Triton
            'output_name': 'output__0'
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏: {e}")
        return {
            'input_shape': [3, 32, 32],
            'output_shape': [10],
            'input_name': 'input__0',
            'output_name': 'output__0'
        }

def analyze_onnx_model():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç ONNX –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    print("\nüîç –ê–Ω–∞–ª–∏–∑ ONNX –º–æ–¥–µ–ª–∏...")
    
    model_path = "models/working_model.onnx"
    if not os.path.exists(model_path):
        print(f"‚ùå ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return None
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º ONNX –º–æ–¥–µ–ª—å
        onnx_model = onnx.load(model_path)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥—ã
        inputs = onnx_model.graph.input
        outputs = onnx_model.graph.output
        
        input_info = {}
        output_info = {}
        
        for inp in inputs:
            name = inp.name
            shape = [dim.dim_value if dim.dim_value > 0 else -1 for dim in inp.type.tensor_type.shape.dim]
            dtype = inp.type.tensor_type.elem_type
            input_info[name] = {'shape': shape, 'dtype': dtype}
            print(f"   Input: {name}, shape: {shape}, dtype: {dtype}")
        
        for out in outputs:
            name = out.name
            shape = [dim.dim_value if dim.dim_value > 0 else -1 for dim in out.type.tensor_type.shape.dim]
            dtype = out.type.tensor_type.elem_type
            output_info[name] = {'shape': shape, 'dtype': dtype}
            print(f"   Output: {name}, shape: {shape}, dtype: {dtype}")
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –≤—Ö–æ–¥/–≤—ã—Ö–æ–¥
        first_input = list(input_info.keys())[0]
        first_output = list(output_info.keys())[0]
        
        input_shape = input_info[first_input]['shape'][1:]  # –£–±–∏—Ä–∞–µ–º batch dimension
        output_shape = output_info[first_output]['shape'][1:]  # –£–±–∏—Ä–∞–µ–º batch dimension
        
        return {
            'input_shape': input_shape,
            'output_shape': output_shape,
            'input_name': first_input,
            'output_name': first_output
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ ONNX: {e}")
        return None

def create_pytorch_config(model_info, model_name):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PyTorch –º–æ–¥–µ–ª–∏"""
    
    config = f'''name: "{model_name}"
platform: "pytorch_libtorch"
max_batch_size: 8
input [
  {{
    name: "{model_info['input_name']}"
    data_type: TYPE_FP32
    dims: [ {", ".join(map(str, model_info['input_shape']))} ]
  }}
]
output [
  {{
    name: "{model_info['output_name']}"
    data_type: TYPE_FP32
    dims: [ {", ".join(map(str, model_info['output_shape']))} ]
  }}
]

instance_group [
  {{
    count: 1
    kind: KIND_CPU
  }}
]

dynamic_batching {{
  max_queue_delay_microseconds: 100
}}

version_policy: {{ all {{ }} }}
'''
    return config

def create_onnx_config(model_info, model_name, optimized=False):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è ONNX –º–æ–¥–µ–ª–∏"""
    
    optimization_block = ""
    if optimized:
        optimization_block = '''
optimization {{
  execution_accelerators {{
    cpu_execution_accelerator : [ {{
      name : "openvino"
    }}]
  }}
}}
'''
    
    instance_count = 2 if optimized else 1
    max_batch = 16 if optimized else 8
    delay = 50 if optimized else 100
    
    preferred_batch = ""
    if optimized:
        preferred_batch = '''
  preferred_batch_size: [ 4, 8 ]'''
    
    config = f'''name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: {max_batch}
input [
  {{
    name: "{model_info['input_name']}"
    data_type: TYPE_FP32
    dims: [ {", ".join(map(str, model_info['input_shape']))} ]
  }}
]
output [
  {{
    name: "{model_info['output_name']}"
    data_type: TYPE_FP32
    dims: [ {", ".join(map(str, model_info['output_shape']))} ]
  }}
]
{optimization_block}
instance_group [
  {{
    count: {instance_count}
    kind: KIND_CPU
  }}
]

dynamic_batching {{
  max_queue_delay_microseconds: {delay}{preferred_batch}
}}

version_policy: {{ all {{ }} }}
'''
    return config

def fix_all_configs():
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    print("üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô TRITON")
    print("=" * 50)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
    pytorch_info = analyze_pytorch_model()
    onnx_info = analyze_onnx_model()
    
    if not pytorch_info:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å PyTorch –º–æ–¥–µ–ª—å")
        return False
    
    if not onnx_info:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å ONNX –º–æ–¥–µ–ª—å")
        return False
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    configs = {
        'cifar10_pytorch': create_pytorch_config(pytorch_info, 'cifar10_pytorch'),
        'cifar10_onnx': create_onnx_config(onnx_info, 'cifar10_onnx', optimized=False),
        'cifar10_onnx_optimized': create_onnx_config(onnx_info, 'cifar10_onnx_optimized', optimized=True)
    }
    
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print("\nüóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    import shutil
    if os.path.exists("triton/model_repository/cifar10_optimized"):
        shutil.rmtree("triton/model_repository/cifar10_optimized")
        print("‚úÖ cifar10_optimized —É–¥–∞–ª–µ–Ω–∞")
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\nüìù –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π...")
    
    for model_name, config_content in configs.items():
        config_path = f"triton/model_repository/{model_name}/config.pbtxt"
        
        if os.path.exists(os.path.dirname(config_path)):
            with open(config_path, 'w') as f:
                f.write(config_content)
            print(f"‚úÖ {config_path} –æ–±–Ω–æ–≤–ª–µ–Ω")
        else:
            print(f"‚ö†Ô∏è {config_path} - –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –±–∞–∑–æ–≤–∞—è ONNX –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    basic_onnx_dir = "triton/model_repository/cifar10_onnx/1"
    if not os.path.exists(f"{basic_onnx_dir}/model.onnx"):
        print("\nüìÇ –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π ONNX –º–æ–¥–µ–ª–∏...")
        os.makedirs(basic_onnx_dir, exist_ok=True)
        
        if os.path.exists("models/working_model.onnx"):
            import shutil
            shutil.copy2("models/working_model.onnx", f"{basic_onnx_dir}/model.onnx")
            print("‚úÖ –ë–∞–∑–æ–≤–∞—è ONNX –º–æ–¥–µ–ª—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞")
    
    print("\nüéâ –í–°–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –ò–°–ü–†–ê–í–õ–ï–ù–´!")
    print("=" * 50)
    
    print("üìã –ì–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è Triton:")
    print("   ‚úÖ cifar10_pytorch - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è PyTorch –º–æ–¥–µ–ª—å")
    print("   ‚úÖ cifar10_onnx - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –±–∞–∑–æ–≤–∞—è ONNX –º–æ–¥–µ–ª—å")
    print("   ‚úÖ cifar10_onnx_optimized - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX")
    print("   ‚ùå cifar10_optimized - —É–¥–∞–ª–µ–Ω–∞ (–ø—Ä–æ–±–ª–µ–º–Ω–∞—è)")
    
    print(f"\nüöÄ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
    print("docker-compose -f docker-compose-triton.yml restart triton")
    
    return True

if __name__ == "__main__":
    success = fix_all_configs()
    
    if success:
        print(f"\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –ø–æ–¥ CIFAR-10!")
        print("   Input: [3, 32, 32] (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è CIFAR-10)")
        print("   Output: [10] (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è 10 –∫–ª–∞—Å—Å–æ–≤)")
        print("   –ò–º–µ–Ω–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤: –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    else:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")