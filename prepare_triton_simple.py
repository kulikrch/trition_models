"""
–ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è Triton
"""
import torch
import torch.nn as nn
import os
import shutil
from working_demo import SimpleCNN

def prepare_triton_models():
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ –¥–ª—è Triton"""
    print("üöÄ –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ï–ô –î–õ–Ø TRITON")
    print("=" * 50)
    
    repo_path = "triton/model_repository"
    models_path = "models"
    
    # 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è PyTorch –º–æ–¥–µ–ª—å
    print("\n1Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π PyTorch –º–æ–¥–µ–ª–∏...")
    pytorch_dir = f"{repo_path}/cifar10_pytorch/1"
    os.makedirs(pytorch_dir, exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ TorchScript
    model = SimpleCNN(num_classes=10)
    checkpoint = torch.load(f"{models_path}/working_model.pth", map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # –°–æ–∑–¥–∞–µ–º traced –º–æ–¥–µ–ª—å
    dummy_input = torch.randn(1, 3, 32, 32)
    traced_model = torch.jit.trace(model, dummy_input)
    traced_model.save(f"{pytorch_dir}/model.pt")
    print(f"‚úÖ PyTorch –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {pytorch_dir}/model.pt")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è PyTorch
    pytorch_config = '''name: "cifar10_pytorch"
platform: "pytorch_libtorch"
max_batch_size: 8
input [
  {
    name: "input__0"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }
]
output [
  {
    name: "output__0"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_CPU
  }
]

dynamic_batching {
  max_queue_delay_microseconds: 100
}

version_policy: { all { }}
'''
    
    with open(f"{repo_path}/cifar10_pytorch/config.pbtxt", 'w') as f:
        f.write(pytorch_config)
    print(f"‚úÖ PyTorch –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    
    # 2. ONNX –º–æ–¥–µ–ª—å
    print("\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ONNX –º–æ–¥–µ–ª–∏...")
    onnx_dir = f"{repo_path}/cifar10_onnx/1"
    os.makedirs(onnx_dir, exist_ok=True)
    
    if os.path.exists(f"{models_path}/working_model.onnx"):
        shutil.copy2(f"{models_path}/working_model.onnx", f"{onnx_dir}/model.onnx")
        print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞: {onnx_dir}/model.onnx")
    else:
        # –°–æ–∑–¥–∞–µ–º ONNX –º–æ–¥–µ–ª—å
        torch.onnx.export(
            model, dummy_input, f"{onnx_dir}/model.onnx",
            export_params=True, opset_version=11, do_constant_folding=True,
            input_names=['input'], output_names=['output']
        )
        print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {onnx_dir}/model.onnx")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è ONNX
    onnx_config = '''name: "cifar10_onnx"
platform: "onnxruntime_onnx"
max_batch_size: 8
input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_CPU
  }
]

dynamic_batching {
  max_queue_delay_microseconds: 100
}

version_policy: { all { }}
'''
    
    with open(f"{repo_path}/cifar10_onnx/config.pbtxt", 'w') as f:
        f.write(onnx_config)
    print(f"‚úÖ ONNX –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    
    # 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (–∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è)
    print("\n3Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    opt_dir = f"{repo_path}/cifar10_optimized/1"
    os.makedirs(opt_dir, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    quantized_model = torch.quantization.quantize_dynamic(
        model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ state_dict (—Ç–∞–∫ –∫–∞–∫ traced quantized –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å)
    torch.save(quantized_model.state_dict(), f"{opt_dir}/model.pt")
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {opt_dir}/model.pt")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    opt_config = '''name: "cifar10_optimized"
platform: "pytorch_libtorch"
max_batch_size: 16
input [
  {
    name: "input__0"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }
]
output [
  {
    name: "output__0"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]

instance_group [
  {
    count: 2
    kind: KIND_CPU
  }
]

dynamic_batching {
  max_queue_delay_microseconds: 50
  preferred_batch_size: [ 4, 8 ]
}

version_policy: { all { }}
'''
    
    with open(f"{repo_path}/cifar10_optimized/config.pbtxt", 'w') as f:
        f.write(opt_config)
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    
    # 4. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX
    print("\n4Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π ONNX –º–æ–¥–µ–ª–∏...")
    onnx_opt_dir = f"{repo_path}/cifar10_onnx_optimized/1"
    os.makedirs(onnx_opt_dir, exist_ok=True)
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º ONNX —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    torch.onnx.export(
        model, dummy_input, f"{onnx_opt_dir}/model.onnx",
        export_params=True, opset_version=11, 
        do_constant_folding=True,  # –í–∫–ª—é—á–∞–µ–º constant folding
        input_names=['input'], output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
    )
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {onnx_opt_dir}/model.onnx")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π ONNX
    onnx_opt_config = '''name: "cifar10_onnx_optimized"
platform: "onnxruntime_onnx"
max_batch_size: 16
input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]

optimization {
  execution_accelerators {
    cpu_execution_accelerator : [ {
      name : "openvino"
    }]
  }
}

instance_group [
  {
    count: 2
    kind: KIND_CPU
  }
]

dynamic_batching {
  max_queue_delay_microseconds: 50
  preferred_batch_size: [ 4, 8, 16 ]
}

version_policy: { all { }}
'''
    
    with open(f"{repo_path}/cifar10_onnx_optimized/config.pbtxt", 'w') as f:
        f.write(onnx_opt_config)
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    
    print(f"\nüéâ –í–°–ï –ú–û–î–ï–õ–ò –ü–û–î–ì–û–¢–û–í–õ–ï–ù–´!")
    print("=" * 50)
    print("üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ model repository:")
    print("   ‚Ä¢ cifar10_pytorch - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è PyTorch –º–æ–¥–µ–ª—å")
    print("   ‚Ä¢ cifar10_onnx - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è ONNX –º–æ–¥–µ–ª—å")  
    print("   ‚Ä¢ cifar10_optimized - –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è PyTorch –º–æ–¥–µ–ª—å")
    print("   ‚Ä¢ cifar10_onnx_optimized - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX –º–æ–¥–µ–ª—å")
    
    return True

if __name__ == "__main__":
    success = prepare_triton_models()
    
    if success:
        print(f"\nüöÄ –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì: –ó–∞–ø—É—Å–∫ Triton Server")
        print("docker-compose -f docker-compose-triton.yml up -d triton")
    else:
        print(f"\n‚ùå –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å")