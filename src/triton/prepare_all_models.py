"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è Triton Inference Server
"""
import torch
import torch.nn as nn
import os
import shutil
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from working_demo import SimpleCNN

class TritonModelPreparer:
    def __init__(self, model_repository_path='../../triton/model_repository'):
        self.model_repository_path = Path(model_repository_path)
        self.models_path = Path('../../models')
        
    def create_model_directories(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        models = [
            'cifar10_original',      # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è PyTorch –º–æ–¥–µ–ª—å
            'cifar10_optimized',     # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è PyTorch –º–æ–¥–µ–ª—å
            'cifar10_onnx',          # ONNX –º–æ–¥–µ–ª—å
            'cifar10_onnx_optimized' # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX –º–æ–¥–µ–ª—å
        ]
        
        for model_name in models:
            model_dir = self.model_repository_path / model_name
            version_dir = model_dir / '1'
            version_dir.mkdir(parents=True, exist_ok=True)
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {model_dir}")
        
        return models
    
    def prepare_pytorch_original(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é PyTorch –º–æ–¥–µ–ª—å"""
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π PyTorch –º–æ–¥–µ–ª–∏...")
        
        model_path = self.models_path / 'working_model.pth'
        if not model_path.exists():
            print(f"‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            return False
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ TorchScript
        model = SimpleCNN(num_classes=10)
        checkpoint = torch.load(model_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # –°–æ–∑–¥–∞–µ–º traced –º–æ–¥–µ–ª—å
        dummy_input = torch.randn(1, 3, 32, 32)
        traced_model = torch.jit.trace(model, dummy_input)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è Triton
        triton_path = self.model_repository_path / 'cifar10_original' / '1' / 'model.pt'
        traced_model.save(str(triton_path))
        
        print(f"‚úÖ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {triton_path}")
        return True
    
    def prepare_pytorch_optimized(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é PyTorch –º–æ–¥–µ–ª—å"""
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π PyTorch –º–æ–¥–µ–ª–∏...")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model_paths = [
            self.models_path / 'working_model_quantized.pth',
            self.models_path / 'working_model_combined.pth',
            self.models_path / 'working_model_pruned.pth'
        ]
        
        source_path = None
        for path in model_paths:
            if path.exists():
                source_path = path
                break
        
        if not source_path:
            print("‚ùå –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        # –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω—É–∂–µ–Ω –æ—Å–æ–±—ã–π –ø–æ–¥—Ö–æ–¥
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
            original_model = SimpleCNN(num_classes=10)
            original_checkpoint = torch.load(self.models_path / 'working_model.pth', map_location='cpu')
            original_model.load_state_dict(original_checkpoint['model_state_dict'])
            original_model.eval()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
            quantized_model = torch.quantization.quantize_dynamic(
                original_model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8
            )
            
            # –°–æ–∑–¥–∞–µ–º traced –º–æ–¥–µ–ª—å (–º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏)
            # –ü–æ—ç—Ç–æ–º—É —Å–æ—Ö—Ä–∞–Ω–∏–º –∫–∞–∫ –æ–±—ã—á–Ω—É—é PyTorch –º–æ–¥–µ–ª—å
            triton_path = self.model_repository_path / 'cifar10_optimized' / '1' / 'model.pt'
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º state_dict –≤–º–µ—Å—Ç–æ traced –º–æ–¥–µ–ª–∏
            torch.save(quantized_model.state_dict(), str(triton_path))
            
            print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {triton_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def prepare_onnx_original(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é ONNX –º–æ–¥–µ–ª—å"""
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π ONNX –º–æ–¥–µ–ª–∏...")
        
        source_path = self.models_path / 'working_model.onnx'
        if not source_path.exists():
            print(f"‚ùå ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {source_path}")
            return False
        
        # –ö–æ–ø–∏—Ä—É–µ–º ONNX –º–æ–¥–µ–ª—å
        triton_path = self.model_repository_path / 'cifar10_onnx' / '1' / 'model.onnx'
        shutil.copy2(source_path, triton_path)
        
        print(f"‚úÖ ONNX –º–æ–¥–µ–ª—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞: {triton_path}")
        return True
    
    def prepare_onnx_optimized(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é ONNX –º–æ–¥–µ–ª—å"""
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π ONNX –º–æ–¥–µ–ª–∏...")
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é ONNX –º–æ–¥–µ–ª—å –∏–∑ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π PyTorch
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            original_model = SimpleCNN(num_classes=10)
            original_checkpoint = torch.load(self.models_path / 'working_model.pth', map_location='cpu')
            original_model.load_state_dict(original_checkpoint['model_state_dict'])
            original_model.eval()
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ ONNX —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
            dummy_input = torch.randn(1, 3, 32, 32)
            triton_path = self.model_repository_path / 'cifar10_onnx_optimized' / '1' / 'model.onnx'
            
            torch.onnx.export(
                original_model,
                dummy_input,
                str(triton_path),
                export_params=True,
                opset_version=11,
                do_constant_folding=True,  # –í–∫–ª—é—á–∞–µ–º constant folding
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                },
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                enable_onnx_checker=True,
                use_external_data_format=False
            )
            
            print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è ONNX –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {triton_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π ONNX: {e}")
            return False
    
    def create_config_files(self):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        configs = {
            'cifar10_original': self.create_pytorch_config('cifar10_original'),
            'cifar10_optimized': self.create_pytorch_config('cifar10_optimized'),
            'cifar10_onnx': self.create_onnx_config('cifar10_onnx'),
            'cifar10_onnx_optimized': self.create_onnx_optimized_config('cifar10_onnx_optimized')
        }
        
        for model_name, config_content in configs.items():
            config_path = self.model_repository_path / model_name / 'config.pbtxt'
            with open(config_path, 'w') as f:
                f.write(config_content)
            print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {config_path}")
        
        return True
    
    def create_pytorch_config(self, model_name):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è PyTorch –º–æ–¥–µ–ª–∏"""
        return f"""name: "{model_name}"
platform: "pytorch_libtorch"
max_batch_size: 16
input [
  {{
    name: "input__0"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }}
]
output [
  {{
    name: "output__0"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }}
]

# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –±–∞—Ç—á–∏–Ω–≥
dynamic_batching {{
  max_queue_delay_microseconds: 100
  preferred_batch_size: [ 4, 8 ]
}}

# –ò–Ω—Å—Ç–∞–Ω—Å—ã –º–æ–¥–µ–ª–∏ –¥–ª—è CPU
instance_group [
  {{
    count: 1
    kind: KIND_CPU
  }}
]

# –í–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
version_policy: {{ all {{ }} }}
"""
    
    def create_onnx_config(self, model_name):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è ONNX –º–æ–¥–µ–ª–∏"""
        return f"""name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: 16
input [
  {{
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }}
]
output [
  {{
    name: "output"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }}
]

# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –±–∞—Ç—á–∏–Ω–≥
dynamic_batching {{
  max_queue_delay_microseconds: 100
  preferred_batch_size: [ 4, 8 ]
}}

# –ò–Ω—Å—Ç–∞–Ω—Å—ã –º–æ–¥–µ–ª–∏ –¥–ª—è CPU
instance_group [
  {{
    count: 1
    kind: KIND_CPU
  }}
]

# –í–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
version_policy: {{ all {{ }} }}
"""
    
    def create_onnx_optimized_config(self, model_name):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π ONNX –º–æ–¥–µ–ª–∏"""
        return f"""name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: 32
input [
  {{
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 32, 32 ]
  }}
]
output [
  {{
    name: "output"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }}
]

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU
optimization {{
  execution_accelerators {{
    cpu_execution_accelerator : [ {{
      name : "openvino"
    }}]
  }}
}}

# –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –±–∞—Ç—á–∏–Ω–≥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
dynamic_batching {{
  max_queue_delay_microseconds: 50
  preferred_batch_size: [ 8, 16, 32 ]
}}

# –ë–æ–ª—å—à–µ –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
instance_group [
  {{
    count: 2
    kind: KIND_CPU
  }}
]

# –í–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
version_policy: {{ all {{ }} }}
"""
    
    def prepare_all_models(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ –¥–ª—è Triton"""
        print("üöÄ –ü–û–î–ì–û–¢–û–í–ö–ê –í–°–ï–• –ú–û–î–ï–õ–ï–ô –î–õ–Ø TRITON")
        print("=" * 60)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        models = self.create_model_directories()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
        results = {}
        
        print(f"\n1Ô∏è‚É£ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø PYTORCH –ú–û–î–ï–õ–¨")
        print("-" * 40)
        results['pytorch_original'] = self.prepare_pytorch_original()
        
        print(f"\n2Ô∏è‚É£ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø PYTORCH –ú–û–î–ï–õ–¨")
        print("-" * 40)
        results['pytorch_optimized'] = self.prepare_pytorch_optimized()
        
        print(f"\n3Ô∏è‚É£ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø ONNX –ú–û–î–ï–õ–¨")
        print("-" * 40)
        results['onnx_original'] = self.prepare_onnx_original()
        
        print(f"\n4Ô∏è‚É£ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø ONNX –ú–û–î–ï–õ–¨")
        print("-" * 40)
        results['onnx_optimized'] = self.prepare_onnx_optimized()
        
        print(f"\n5Ô∏è‚É£ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –§–ê–ô–õ–´")
        print("-" * 40)
        results['configs'] = self.create_config_files()
        
        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        print(f"\nüéâ –ü–û–î–ì–û–¢–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 40)
        
        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {success_count}/{total_count}")
        
        for name, success in results.items():
            status = "‚úÖ" if success else "‚ùå"
            print(f"  {status} {name}")
        
        if success_count >= 3:  # –ú–∏–Ω–∏–º—É–º 3 –º–æ–¥–µ–ª–∏ –∏–∑ 5
            print(f"\nüöÄ –ì–æ—Ç–æ–≤–æ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Triton!")
            print(f"üìÇ Model repository: {self.model_repository_path}")
            return True
        else:
            print(f"\n‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞")
            return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    preparer = TritonModelPreparer()
    success = preparer.prepare_all_models()
    
    if success:
        print(f"\nüìã –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("1. –ó–∞–ø—É—Å—Ç–∏—Ç—å Triton: docker-compose up triton")
        print("2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª–∏: curl http://localhost:8001/v2/models")
        print("3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ API")
    
    return success

if __name__ == "__main__":
    main()