"""
ะขะตััะธัะพะฒะฐะฝะธะต ะฒัะตั ะผะพะดะตะปะตะน ะฒ Triton Inference Server
"""
import tritonclient.grpc as grpcclient
import tritonclient.http as httpclient
import numpy as np
import time
import sys
import os

class TritonTester:
    def __init__(self, triton_url="triton:8001", use_grpc=True):
        self.triton_url = triton_url
        self.use_grpc = use_grpc
        
        # ะะฝะธัะธะฐะปะธะทะฐัะธั ะบะปะธะตะฝัะฐ
        if use_grpc:
            self.client = grpcclient.InferenceServerClient(url=triton_url)
        else:
            http_url = triton_url.replace(':8001', ':8002')
            self.client = httpclient.InferenceServerClient(url=http_url)
        
        # CIFAR-10 ะบะปะฐััั
        self.class_names = [
            'airplane', 'automobile', 'bird', 'cat', 'deer',
            'dog', 'frog', 'horse', 'ship', 'truck'
        ]
    
    def wait_for_triton(self, max_retries=30):
        """ะะดะตั ะณะพัะพะฒะฝะพััะธ Triton ัะตัะฒะตัะฐ"""
        print("โณ ะะถะธะดะฐะฝะธะต ะณะพัะพะฒะฝะพััะธ Triton ัะตัะฒะตัะฐ...")
        
        for i in range(max_retries):
            try:
                if self.client.is_server_live():
                    if self.client.is_server_ready():
                        print("โ Triton ัะตัะฒะตั ะณะพัะพะฒ!")
                        return True
                    else:
                        print(f"   ะกะตัะฒะตั ะทะฐะฟััะตะฝ, ะฝะพ ะฝะต ะณะพัะพะฒ... ({i+1}/{max_retries})")
                else:
                    print(f"   ะกะตัะฒะตั ะฝะต ะพัะฒะตัะฐะตั... ({i+1}/{max_retries})")
            except Exception as e:
                print(f"   ะัะธะฑะบะฐ ะฟะพะดะบะปััะตะฝะธั: {e} ({i+1}/{max_retries})")
            
            time.sleep(5)
        
        print("โ Triton ัะตัะฒะตั ะฝะต ะณะพัะพะฒ")
        return False
    
    def list_models(self):
        """ะะพะปััะฐะตั ัะฟะธัะพะบ ะดะพัััะฟะฝัั ะผะพะดะตะปะตะน"""
        try:
            models = self.client.get_model_repository_index()
            print("๐ ะะพัััะฟะฝัะต ะผะพะดะตะปะธ:")
            
            available_models = []
            for model in models:
                name = model['name'] if isinstance(model, dict) else model.name
                state = model['state'] if isinstance(model, dict) else model.state
                print(f"   โข {name}: {state}")
                
                if state == 'READY':
                    available_models.append(name)
            
            return available_models
            
        except Exception as e:
            print(f"โ ะัะธะฑะบะฐ ะฟะพะปััะตะฝะธั ัะฟะธัะบะฐ ะผะพะดะตะปะตะน: {e}")
            return []
    
    def get_model_metadata(self, model_name):
        """ะะพะปััะฐะตั ะผะตัะฐะดะฐะฝะฝัะต ะผะพะดะตะปะธ"""
        try:
            metadata = self.client.get_model_metadata(model_name)
            
            print(f"๐ ะะตัะฐะดะฐะฝะฝัะต ะผะพะดะตะปะธ {model_name}:")
            if hasattr(metadata, 'platform'):
                print(f"   Platform: {metadata.platform}")
            if hasattr(metadata, 'inputs'):
                for inp in metadata.inputs:
                    print(f"   Input: {inp.name}, shape: {inp.shape}, dtype: {inp.datatype}")
            if hasattr(metadata, 'outputs'):
                for out in metadata.outputs:
                    print(f"   Output: {out.name}, shape: {out.shape}, dtype: {out.datatype}")
            
            return metadata
            
        except Exception as e:
            print(f"โ ะัะธะฑะบะฐ ะฟะพะปััะตะฝะธั ะผะตัะฐะดะฐะฝะฝัั: {e}")
            return None
    
    def create_test_data(self, batch_size=1):
        """ะกะพะทะดะฐะตั ัะตััะพะฒัะต ะดะฐะฝะฝัะต"""
        # ะกะพะทะดะฐะตะผ ัะปััะฐะนะฝัะต ะดะฐะฝะฝัะต ะฒ ัะพัะผะฐัะต CIFAR-10
        data = np.random.randn(batch_size, 3, 32, 32).astype(np.float32)
        
        # ะะพัะผะฐะปะธะทะฐัะธั ะบะฐะบ ะฒ ะพะฑััะตะฝะธะธ
        mean = np.array([0.4914, 0.4822, 0.4465]).reshape(1, 3, 1, 1)
        std = np.array([0.2023, 0.1994, 0.2010]).reshape(1, 3, 1, 1)
        data = (data - mean) / std
        
        return data
    
    def test_model_inference(self, model_name, num_tests=10):
        """ะขะตััะธััะตั ะธะฝัะตัะตะฝั ะผะพะดะตะปะธ"""
        print(f"๐งช ะขะตััะธัะพะฒะฐะฝะธะต ะผะพะดะตะปะธ: {model_name}")
        
        # ะะพะปััะฐะตะผ ะผะตัะฐะดะฐะฝะฝัะต
        metadata = self.get_model_metadata(model_name)
        if not metadata:
            return None
        
        try:
            # ะะฟัะตะดะตะปัะตะผ ะธะผะตะฝะฐ ะฒัะพะดะพะฒ ะธ ะฒััะพะดะพะฒ
            if hasattr(metadata, 'inputs') and len(metadata.inputs) > 0:
                input_name = metadata.inputs[0].name
                input_shape = metadata.inputs[0].shape
            else:
                input_name = 'input'
                input_shape = [3, 32, 32]
            
            if hasattr(metadata, 'outputs') and len(metadata.outputs) > 0:
                output_name = metadata.outputs[0].name
            else:
                output_name = 'output'
            
            print(f"   Input: {input_name}, Output: {output_name}")
            
            # ะกะพะทะดะฐะตะผ ัะตััะพะฒัะต ะดะฐะฝะฝัะต
            test_data = self.create_test_data(batch_size=1)
            
            # ะะพะดะณะพัะฐะฒะปะธะฒะฐะตะผ ะทะฐะฟัะพั
            if self.use_grpc:
                inputs = [grpcclient.InferInput(input_name, test_data.shape, "FP32")]
                inputs[0].set_data_from_numpy(test_data)
                outputs = [grpcclient.InferRequestedOutput(output_name)]
            else:
                inputs = [httpclient.InferInput(input_name, test_data.shape, "FP32")]
                inputs[0].set_data_from_numpy(test_data)
                outputs = [httpclient.InferRequestedOutput(output_name)]
            
            # ะัะพะณัะตะฒ
            for _ in range(3):
                try:
                    response = self.client.infer(model_name, inputs, outputs=outputs)
                except:
                    pass
            
            # ะะทะผะตัะตะฝะธะต ะฟัะพะธะทะฒะพะดะธัะตะปัะฝะพััะธ
            times = []
            results = []
            
            for i in range(num_tests):
                start_time = time.time()
                
                try:
                    response = self.client.infer(model_name, inputs, outputs=outputs)
                    end_time = time.time()
                    
                    # ะะพะปััะฐะตะผ ัะตะทัะปััะฐั
                    if self.use_grpc:
                        output_data = response.as_numpy(output_name)
                    else:
                        output_data = response.as_numpy(output_name)
                    
                    times.append(end_time - start_time)
                    
                    # ะะฝะฐะปะธะทะธััะตะผ ะฟัะตะดัะบะฐะทะฐะฝะธะต
                    predicted_class = np.argmax(output_data[0])
                    confidence = np.max(np.softmax(output_data[0]))
                    results.append((predicted_class, confidence))
                    
                except Exception as e:
                    print(f"   โ ะัะธะฑะบะฐ ะฒ ัะตััะต {i+1}: {e}")
                    continue
            
            if times:
                avg_time = np.mean(times) * 1000  # ms
                std_time = np.std(times) * 1000
                min_time = np.min(times) * 1000
                max_time = np.max(times) * 1000
                throughput = 1000 / avg_time  # fps
                
                # ะะพัะปะตะดะฝะธะน ัะตะทัะปััะฐั ะดะปั ะฟัะธะผะตัะฐ
                if results:
                    last_class, last_conf = results[-1]
                    class_name = self.class_names[last_class]
                
                print(f"   ๐ ะะตะทัะปััะฐัั ({len(times)} ััะฟะตัะฝัั ัะตััะพะฒ):")
                print(f"      ะัะตะผั: {avg_time:.2f} ยฑ {std_time:.2f} ms")
                print(f"      ะะธะฐะฟะฐะทะพะฝ: {min_time:.2f} - {max_time:.2f} ms")
                print(f"      Throughput: {throughput:.1f} fps")
                if results:
                    print(f"      ะัะธะผะตั: {class_name} ({last_conf:.3f})")
                
                return {
                    'model_name': model_name,
                    'successful_tests': len(times),
                    'avg_time_ms': avg_time,
                    'std_time_ms': std_time,
                    'min_time_ms': min_time,
                    'max_time_ms': max_time,
                    'throughput_fps': throughput,
                    'sample_prediction': class_name if results else None,
                    'sample_confidence': last_conf if results else None
                }
            else:
                print(f"   โ ะัะต ัะตััั ะฝะตัะดะฐัะฝั")
                return None
                
        except Exception as e:
            print(f"โ ะัะธะฑะบะฐ ัะตััะธัะพะฒะฐะฝะธั ะผะพะดะตะปะธ {model_name}: {e}")
            return None
    
    def run_comprehensive_test(self):
        """ะะฐะฟััะบะฐะตั ะบะพะผะฟะปะตะบัะฝะพะต ัะตััะธัะพะฒะฐะฝะธะต ะฒัะตั ะผะพะดะตะปะตะน"""
        print("๐ ะะะะะะะะกะะะ ะขะะกะขะะะะะะะะ TRITON INFERENCE SERVER")
        print("=" * 70)
        
        # ะัะพะฒะตััะตะผ ะณะพัะพะฒะฝะพััั ัะตัะฒะตัะฐ
        if not self.wait_for_triton():
            return False
        
        # ะะพะปััะฐะตะผ ัะฟะธัะพะบ ะผะพะดะตะปะตะน
        available_models = self.list_models()
        if not available_models:
            print("โ ะะตั ะดะพัััะฟะฝัั ะผะพะดะตะปะตะน")
            return False
        
        print(f"\n๐ฏ ะะฐะนะดะตะฝะพ {len(available_models)} ะณะพัะพะฒัั ะผะพะดะตะปะตะน")
        
        # ะขะตััะธััะตะผ ะบะฐะถะดัั ะผะพะดะตะปั
        results = []
        
        for i, model_name in enumerate(available_models, 1):
            print(f"\n{i}๏ธโฃ {'='*50}")
            print(f"{i}๏ธโฃ ะขะะกะขะะะะะะะะ: {model_name.upper()}")
            print(f"{i}๏ธโฃ {'='*50}")
            
            result = self.test_model_inference(model_name, num_tests=20)
            if result:
                results.append(result)
        
        # ะัะพะณะพะฒะฐั ัะฒะพะดะบะฐ
        if results:
            self.print_summary_table(results)
        
        return len(results) > 0
    
    def print_summary_table(self, results):
        """ะะตัะฐัะฐะตั ะธัะพะณะพะฒัั ัะฐะฑะปะธัั ัะตะทัะปััะฐัะพะฒ"""
        print("\n๐ " + "="*80 + " ๐")
        print("๐" + " "*25 + "ะะขะะะะะซะ ะะะะฃะะฌะขะะขะซ TRITON" + " "*25 + "๐")
        print("๐ " + "="*80 + " ๐")
        
        # ะะฐะณะพะปะพะฒะพะบ ัะฐะฑะปะธัั
        print(f"{'ะะพะดะตะปั':<25} {'ะัะตะผั (ms)':<12} {'Throughput':<12} {'ะขะตััะพะฒ':<8}")
        print("-" * 70)
        
        # ะะตะทัะปััะฐัั
        for result in results:
            name = result['model_name']
            time_ms = f"{result['avg_time_ms']:.2f}"
            throughput = f"{result['throughput_fps']:.1f} fps"
            tests = result['successful_tests']
            
            print(f"{name:<25} {time_ms:<12} {throughput:<12} {tests:<8}")
        
        # ะัััะธะต ัะตะทัะปััะฐัั
        print(f"\n๐ ะะฃะงะจะะ ะะะะฃะะฌะขะะขะซ:")
        print("-" * 30)
        
        fastest = min(results, key=lambda x: x['avg_time_ms'])
        highest_throughput = max(results, key=lambda x: x['throughput_fps'])
        
        print(f"๐ ะกะฐะผะฐั ะฑััััะฐั: {fastest['model_name']} ({fastest['avg_time_ms']:.2f} ms)")
        print(f"๐ ะะฐะธะฒัััะธะน throughput: {highest_throughput['model_name']} ({highest_throughput['throughput_fps']:.1f} fps)")

def main():
    """ะะปะฐะฒะฝะฐั ััะฝะบัะธั"""
    triton_url = os.getenv('TRITON_URL', 'triton:8001')
    
    print(f"๐ ะะพะดะบะปััะตะฝะธะต ะบ Triton: {triton_url}")
    
    # ะขะตััะธััะตะผ gRPC
    print(f"\n๐ก ะขะะกะขะะะะะะะะ gRPC ะะะขะะะคะะะกะ")
    print("-" * 40)
    
    try:
        grpc_tester = TritonTester(triton_url, use_grpc=True)
        grpc_success = grpc_tester.run_comprehensive_test()
    except Exception as e:
        print(f"โ gRPC ัะตััะธัะพะฒะฐะฝะธะต ะฝะตัะดะฐัะฝะพ: {e}")
        grpc_success = False
    
    # ะขะตััะธััะตะผ HTTP (ะตัะปะธ gRPC ะฝะต ัะฐะฑะพัะฐะตั)
    if not grpc_success:
        print(f"\n๐ ะขะะกะขะะะะะะะะ HTTP ะะะขะะะคะะะกะ")
        print("-" * 40)
        
        try:
            http_url = triton_url.replace(':8001', ':8002')
            http_tester = TritonTester(http_url, use_grpc=False)
            http_success = http_tester.run_comprehensive_test()
        except Exception as e:
            print(f"โ HTTP ัะตััะธัะพะฒะฐะฝะธะต ะฝะตัะดะฐัะฝะพ: {e}")
            http_success = False
    else:
        http_success = True
    
    # ะัะพะณะพะฒัะน ัะตะทัะปััะฐั
    if grpc_success or http_success:
        print(f"\nโ TRITON INFERENCE SERVER ะะะะขะะกะขะะะะะะ ะฃะกะะะจะะ!")
        return True
    else:
        print(f"\nโ ะขะะกะขะะะะะะะะ TRITON ะะะฃะะะงะะ")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)