"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ–≥–æ ML –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞
"""
import os
import sys
import subprocess
import time
import argparse
from pathlib import Path


class MLProjectRunner:
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.models_dir = self.project_root / "models"
        self.models_dir.mkdir(exist_ok=True)
        
    def run_command(self, command, cwd=None, shell=True):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        print(f"Running: {command}")
        try:
            if cwd:
                result = subprocess.run(command, shell=shell, cwd=cwd, check=True, 
                                      capture_output=True, text=True)
            else:
                result = subprocess.run(command, shell=shell, check=True, 
                                      capture_output=True, text=True)
            print(f"‚úì Command completed successfully")
            return True, result.stdout
        except subprocess.CalledProcessError as e:
            print(f"‚úó Command failed: {e}")
            print(f"Error output: {e.stderr}")
            return False, e.stderr
    
    def check_dependencies(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
        print("Checking dependencies...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python –ø–∞–∫–µ—Ç—ã
        required_packages = [
            'torch', 'torchvision', 'onnx', 'onnxruntime', 
            'fastapi', 'uvicorn', 'tritonclient', 'prometheus_client'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
                print(f"‚úì {package} is installed")
            except ImportError:
                missing_packages.append(package)
                print(f"‚úó {package} is missing")
        
        if missing_packages:
            print(f"Installing missing packages: {missing_packages}")
            success, _ = self.run_command(f"pip install {' '.join(missing_packages)}")
            if not success:
                print("Failed to install dependencies")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Docker
        success, _ = self.run_command("docker --version")
        if not success:
            print("‚úó Docker is not installed or not running")
            return False
        print("‚úì Docker is available")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Docker Compose
        success, _ = self.run_command("docker-compose --version")
        if not success:
            print("‚úó Docker Compose is not installed")
            return False
        print("‚úì Docker Compose is available")
        
        return True
    
    def train_model(self, epochs=10):  # –£–º–µ–Ω—å—à–∞–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è CPU
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        print(f"\n{'='*60}")
        print("STEP 1: TRAINING MODEL")
        print(f"{'='*60}")
        
        if os.path.exists(self.models_dir / "cifar10_model.pth"):
            response = input("Model already exists. Retrain? (y/N): ")
            if response.lower() != 'y':
                print("Skipping training")
                return True
        
        train_script = self.project_root / "src" / "train" / "train.py"
        success, output = self.run_command(f"python {train_script}", cwd=self.project_root)
        
        if success:
            print("‚úì Model training completed")
            return True
        else:
            print("‚úó Model training failed")
            return False
    
    def convert_to_onnx(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –≤ ONNX"""
        print(f"\n{'='*60}")
        print("STEP 2: CONVERTING TO ONNX")
        print(f"{'='*60}")
        
        if os.path.exists(self.models_dir / "cifar10_model.onnx"):
            response = input("ONNX model already exists. Reconvert? (y/N): ")
            if response.lower() != 'y':
                print("Skipping ONNX conversion")
                return True
        
        convert_script = self.project_root / "src" / "convert" / "to_onnx.py"
        success, output = self.run_command(f"python {convert_script}", cwd=self.project_root)
        
        if success:
            print("‚úì ONNX conversion completed")
            return True
        else:
            print("‚úó ONNX conversion failed")
            return False
    
    def optimize_models(self):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏"""
        print(f"\n{'='*60}")
        print("STEP 3: OPTIMIZING MODELS")
        print(f"{'='*60}")
        
        optimize_script = self.project_root / "src" / "optimize" / "pytorch_optimize.py"
        success, output = self.run_command(f"python {optimize_script}", cwd=self.project_root)
        
        if success:
            print("‚úì Model optimization completed")
            return True
        else:
            print("‚úó Model optimization failed")
            return False
    
    def prepare_triton_models(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è Triton"""
        print(f"\n{'='*60}")
        print("STEP 4: PREPARING TRITON MODELS")
        print(f"{'='*60}")
        
        prepare_script = self.project_root / "src" / "convert" / "prepare_triton.py"
        success, output = self.run_command(f"python {prepare_script}", cwd=self.project_root)
        
        if success:
            print("‚úì Triton model preparation completed")
            return True
        else:
            print("‚úó Triton model preparation failed")
            return False
    
    def start_services(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã"""
        print(f"\n{'='*60}")
        print("STEP 5: STARTING SERVICES")
        print(f"{'='*60}")
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        print("Stopping existing containers...")
        self.run_command("docker-compose down", cwd=self.project_root)
        
        # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–∑—ã
        print("Building Docker images...")
        success, _ = self.run_command("docker-compose build", cwd=self.project_root)
        if not success:
            print("‚úó Failed to build Docker images")
            return False
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å—ã
        print("Starting services...")
        success, _ = self.run_command("docker-compose up -d", cwd=self.project_root)
        if not success:
            print("‚úó Failed to start services")
            return False
        
        print("‚úì Services started successfully")
        print("Waiting for services to be ready...")
        
        # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
        max_wait = 120  # 2 –º–∏–Ω—É—Ç—ã
        for i in range(max_wait):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Triton
            success_triton, _ = self.run_command(
                "curl -f http://localhost:8000/v2/health/ready", 
                cwd=self.project_root
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º preprocessing service
            success_preprocess, _ = self.run_command(
                "curl -f http://localhost:8080/health", 
                cwd=self.project_root
            )
            
            if success_triton and success_preprocess:
                print("‚úì All services are ready!")
                return True
            
            time.sleep(5)
            if (i + 1) % 6 == 0:
                print(f"Still waiting... ({i+1}/{max_wait})")
        
        print("‚úó Services did not become ready in time")
        return False
    
    def run_tests(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã"""
        print(f"\n{'='*60}")
        print("STEP 6: RUNNING TESTS")
        print(f"{'='*60}")
        
        # –¢–µ—Å—Ç preprocessing service
        print("Testing preprocessing service...")
        test_script = self.project_root / "src" / "preprocess_service" / "test_service.py"
        success, _ = self.run_command(f"python {test_script} http://localhost:8080", cwd=self.project_root)
        
        if not success:
            print("‚úó Preprocessing service tests failed")
            return False
        
        # –¢–µ—Å—Ç inference —á–µ—Ä–µ–∑ Triton
        print("Testing inference pipeline...")
        success, _ = self.run_command(
            "docker-compose run --rm test-client", 
            cwd=self.project_root
        )
        
        if success:
            print("‚úì All tests passed")
            return True
        else:
            print("‚úó Some tests failed")
            return False
    
    def show_status(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤"""
        print(f"\n{'='*60}")
        print("SERVICE STATUS")
        print(f"{'='*60}")
        
        services = [
            ("Triton HTTP", "http://localhost:8000/v2/health/ready"),
            ("Triton Metrics", "http://localhost:8002/metrics"),
            ("Preprocessing Service", "http://localhost:8080/health"),
            ("Prometheus", "http://localhost:9090"),
            ("Grafana", "http://localhost:3000")
        ]
        
        for name, url in services:
            success, _ = self.run_command(f"curl -f {url}")
            status = "‚úì Running" if success else "‚úó Not responding"
            print(f"{name:<25} {status}")
        
        print("\nAccess URLs:")
        print("- Triton Server: http://localhost:8000")
        print("- Preprocessing Service: http://localhost:8080")
        print("- Prometheus: http://localhost:9090")
        print("- Grafana: http://localhost:3000 (admin/admin)")
        
        print("\nTo view logs: docker-compose logs <service-name>")
        print("To stop services: docker-compose down")
    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–µ–∫—Ç–µ"""
        print(f"\n{'='*60}")
        print("GENERATING PROJECT REPORT")
        print(f"{'='*60}")
        
        report_content = f"""# ML Deployment Project Report

## Project Overview
–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ—Ç –æ–±—É—á–µ–Ω–∏—è –¥–æ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞.

## Completed Steps

### 1. Model Training
- ‚úì –û–±—É—á–µ–Ω–∞ CNN –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ CIFAR-10
- ‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: `{self.models_dir}/cifar10_model.pth`

### 2. Model Conversion & Optimization
- ‚úì –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ONNX: `{self.models_dir}/cifar10_model.onnx`
- ‚úì –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: `{self.models_dir}/cifar10_model_quantized.pth`
- ‚úì –ü—Ä—É–Ω–∏–Ω–≥ –º–æ–¥–µ–ª–∏: `{self.models_dir}/cifar10_model_pruned.pth`
- ‚úì –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: `{self.models_dir}/cifar10_model_combined.pth`

### 3. Microservice Architecture
- ‚úì FastAPI –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- ‚úì Triton Inference Server –¥–ª—è –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
- ‚úì Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### 4. Monitoring & Observability
- ‚úì Prometheus –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
- ‚úì Grafana –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
- ‚úì Health checks –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤

### 5. Deployment & Orchestration
- ‚úì Docker Compose –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏
- ‚úì –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
- ‚úì CI/CD –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client App    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Preprocess API   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Triton Server   ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ (FastAPI)        ‚îÇ    ‚îÇ (ONNX/PyTorch)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ   Prometheus     ‚îÇ    ‚îÇ   GPU/CPU       ‚îÇ
                       ‚îÇ   (Metrics)      ‚îÇ    ‚îÇ   Inference     ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ     Grafana      ‚îÇ
                       ‚îÇ   (Dashboard)    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Performance Results

–°–º. –ª–æ–≥–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

## Access Points

- **Triton Server**: http://localhost:8000
- **Preprocessing API**: http://localhost:8080
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)

## Usage Examples

### 1. Preprocessing API
```bash
curl -X POST -F "file=@image.jpg" http://localhost:8080/preprocess/single
```

### 2. Direct Triton Inference
```python
import tritonclient.http as httpclient
client = httpclient.InferenceServerClient(url="localhost:8000")
# ... (—Å–º. src/client/test_inference.py –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞)
```

## File Structure

```
{self.project_root}/
‚îú‚îÄ‚îÄ models/                    # –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ train/                # –ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ convert/              # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
‚îÇ   ‚îú‚îÄ‚îÄ optimize/             # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ preprocess_service/   # FastAPI –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å
‚îÇ   ‚îî‚îÄ‚îÄ client/               # –ö–ª–∏–µ–Ω—Ç—Å–∫–∏–µ —Å–∫—Ä–∏–ø—Ç—ã
‚îú‚îÄ‚îÄ triton/
‚îÇ   ‚îî‚îÄ‚îÄ model_repository/     # –ú–æ–¥–µ–ª–∏ –¥–ª—è Triton
‚îú‚îÄ‚îÄ monitoring/               # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚îú‚îÄ‚îÄ docker/                   # Dockerfile'—ã
‚îî‚îÄ‚îÄ docker-compose.yml        # –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è
```

## Next Steps

1. –î–æ–±–∞–≤–∏—Ç—å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
2. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
3. –î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
4. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å Kubernetes
5. –î–æ–±–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥—Ä–∏—Ñ—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ü—Ä–æ–µ–∫—Ç —É—Å–ø–µ—à–Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é ML –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º:
- –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏–∏ (Docker)
- –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (Triton)
- –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∞.
"""
        
        report_path = self.project_root / "PROJECT_REPORT.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"‚úì Project report generated: {report_path}")
        return True
    
    def run_full_pipeline(self, epochs=10):  # –£–º–µ–Ω—å—à–∞–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è CPU
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–æ–µ–∫—Ç–∞"""
        print("="*80)
        print("STARTING FULL ML DEPLOYMENT PIPELINE")
        print("="*80)
        
        steps = [
            ("Checking dependencies", self.check_dependencies),
            ("Training model", lambda: self.train_model(epochs)),
            ("Converting to ONNX", self.convert_to_onnx),
            ("Optimizing models", self.optimize_models),
            ("Preparing Triton models", self.prepare_triton_models),
            ("Starting services", self.start_services),
            ("Running tests", self.run_tests),
            ("Generating report", self.generate_report)
        ]
        
        for step_name, step_func in steps:
            print(f"\n{'='*20} {step_name.upper()} {'='*20}")
            success = step_func()
            
            if not success:
                print(f"\n‚ùå PIPELINE FAILED AT: {step_name}")
                print("Check the logs above for details.")
                return False
        
        print(f"\n{'='*80}")
        print("üéâ PIPELINE COMPLETED SUCCESSFULLY!")
        print(f"{'='*80}")
        
        self.show_status()
        
        return True


def main():
    parser = argparse.ArgumentParser(description='ML Project Pipeline Runner')
    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs (default: 10 for CPU)')
    parser.add_argument('--skip-training', action='store_true', help='Skip model training')
    parser.add_argument('--status-only', action='store_true', help='Only show service status')
    parser.add_argument('--stop', action='store_true', help='Stop all services')
    
    args = parser.parse_args()
    
    runner = MLProjectRunner()
    
    if args.stop:
        print("Stopping all services...")
        runner.run_command("docker-compose down", cwd=runner.project_root)
        return
    
    if args.status_only:
        runner.show_status()
        return
    
    if args.skip_training:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
        if not os.path.exists(runner.models_dir / "cifar10_model.pth"):
            print("‚ùå Model not found and training is skipped!")
            print("Either train the model first or remove --skip-training flag")
            sys.exit(1)
        
        print("Skipping training, starting from conversion...")
        steps = [
            ("Checking dependencies", runner.check_dependencies),
            ("Converting to ONNX", runner.convert_to_onnx),
            ("Optimizing models", runner.optimize_models),
            ("Preparing Triton models", runner.prepare_triton_models),
            ("Starting services", runner.start_services),
            ("Running tests", runner.run_tests),
            ("Generating report", runner.generate_report)
        ]
        
        for step_name, step_func in steps:
            print(f"\n{'='*20} {step_name.upper()} {'='*20}")
            success = step_func()
            
            if not success:
                print(f"\n‚ùå PIPELINE FAILED AT: {step_name}")
                sys.exit(1)
        
        runner.show_status()
    else:
        success = runner.run_full_pipeline(epochs=args.epochs)
        if not success:
            sys.exit(1)


if __name__ == "__main__":
    main()