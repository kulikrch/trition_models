version: '3.8'

services:
  # Triton Inference Server
  triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: triton-server
    ports:
      - "8001:8001"  # gRPC
      - "8002:8002"  # HTTP
      - "8003:8003"  # Metrics
    volumes:
      - ./triton/model_repository:/models
    command: tritonserver --model-repository=/models --log-verbose=1 --log-info=true --log-warning=true --log-error=true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Triton Client для тестирования
  triton-client:
    build:
      context: .
      dockerfile: docker/Dockerfile.tritonclient
    container_name: triton-client
    depends_on:
      - triton
    environment:
      - TRITON_URL=triton:8001
    volumes:
      - ./models:/models:ro
    profiles:
      - testing

networks:
  default:
    name: triton-network